\section{Dynamic Hash Table}\label{sec:dyn}
In this section, we propose a resizing strategy against dynamic hash table updates on GPUs. We first present the hash table design in Section~\ref{sec:dyn:has}.
Subsequently, the resizing strategy is introduced in Section~\ref{sec:dyn:resize}.
In Section~\ref{sec:dyn:distribute}, we discuss how to distribute KV pairs for better load balancing with theoretical guarantees. 
Lastly, we present how to efficiently rehash and relocate data after the tables have been resized in Section~\ref{sec:dyn:rehash}. 

\subsection{Hash Table Structure}\label{sec:dyn:has}
Following cuckoo hashing \cite{pagh2004cuckoo}, we build $d$ hash tables with $d$ unique hash functions: $h^1,h^2,\ldots,h^d$. 
In this work, we use a set of simple universal hash functions such as $h^i(k) = (a_i\cdot k + b_i \mod p) \mod |h^i|$.
Here $a_i,b_i$ are random integers and $p$ is a large prime.
The proposed approaches in this paper also apply to other hash functions as well. 
There are three major advantages of adopting cuckoo hashing on GPUs. 
First, it avoids chaining by inserting the elements into alternative locations if collisions occur. As discussed in Section~\ref{sec:rel}, 
chaining presents several issues that are not friendly to GPU architecture.  
Second, to look up a KV pair, one searches only $d$ locations as specified by $d$ unique hash functions. 
Thus, the data could be stored contiguously in the same location to enable preferred coalesced memory access. 
Third, cuckoo hashing can maintain a high filled factor, which is ideal for saving memory in dynamic scenarios. 
For $d=3$, cuckoo hashing achieves a filled factor of more than 90\% and still efficiently processes \formal{insert} operations \cite{fotakis2005space}.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{fig/Hashtable.pdf}
	\caption{The hash table structure}
	\label{fig:hashtable}
\end{figure}

Figure~\ref{fig:hashtable} depicts the design of a single hash table $h^i$ on GPUs. 
The keys are assumed to be 4-byte integers and a bucket of 32 keys, which are all hashed to the same value $h^i_j$, are stored consecutively in the memory. 
The design of buckets maximizes memory bandwidth utilization in GPUs. 
Consider that the L1 cache line size is 128 bytes. Only a single access is required when one warp is assigned to access a bucket. 
The values associated with the keys in the same bucket are also stored consecutively, but in a separate array.  
In other words, we use two arrays, one to store the keys and one to store the values respectively.
However, the values can take up a much larger memory space than the keys; therefore storing keys and values separately avoids memory access overhead when it is not necessary to access the values, such as when finding a nonexistent KV pair or deleting a KV pair. 

For keys larger than 4 bytes, a simple strategy is to store fewer KV pairs in a bucket. If keys are 8 bytes, a bucket can then accommodate 16 KV pairs. 
Furthermore, we lock the entire bucket exclusively for a warp to perform insertions and deletions using intra warp synchronization primitives. Thus, we do not limit ourselves to supporting KV pairs with only 64 bits. 
In the worst case, a key taking 128 bytes would occupy one bucket, which is unnecessarily large in practice.

\subsection{Structure Resizing}\label{sec:dyn:resize}
To efficiently utilize GPU memory, we resize the hash tables when the filled factor falls out of the desired range $[\alpha,\beta]$.
One possible strategy to address this is to double or half all hash tables and rehash all KV pairs. However, this simple strategy renders poor memory utilization and 
excessive rehashing overhead. First, doubling hash table size results in the filled factor being immediately cut in half, whereas downsizing hash tables to half the original size followed by rehashing is only efficient when the filled factor is significantly low (e.g., $40\%$). Both scenarios are not resource friendly. Second, rehashing all KV pairs is expensive and it harms the performance stability for most streaming applications as the entire table is subject to locking. 

Thus, we propose an alternative strategy, illustrated in Figure~\ref{fig:example-resize}.
Given $d$ hash tables,
we always double the smallest subtable or chop the largest subtable in half for upsizing or downsizing, respectively, when the filled factor falls out of the desired range. 
In other words, no subtable will be more than twice the size of others. This strategy implies that we do not need to lock all hash tables to resize only one, thus achieving better performance stability than the aforementioned simple strategy. 


\vspace{1mm}
\noindent\textbf{Filled factor analysis:}
Assuming there are $d'$ hash tables with size $2n$, $d-d'$ tables with size $n$ and a current filled factor of $\theta$, 
one upsizing process when $\theta > \beta$ lowers the filled factor to $\frac{\theta\cdot(d+d')}{d+d'+1} \geq \frac{\beta \cdot d}{d+1}$.  
Because the filled factor is always lower bounded by $\alpha$, we can deduce that $\alpha < \frac{d}{d+1}$.
Apparently, a higher lower bound can be achieved by adding more hash tables, although it leads to less efficient \formal{find} and \formal{delete} operations. 
We allow the user to configure the number of hash tables to trade off memory and query processing efficiency. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.32\textwidth]{fig/MultiTable.pdf}
	\caption{The resizing strategy}
	\label{fig:example-resize}
\end{figure}
\subsection{KV distribution}\label{sec:dyn:distribute}
Given a set of KV pairs to insert in parallel, it is critical to distribute those KV pairs among the hash tables in a way that minimizes hash collisions to reduce the corresponding thread conflicts. We have the following theorem to guide us in distributing KV pairs. 

\begin{theorem}\label{them:balance}
	The amortized conflicts for inserting $m$ unique KV pairs to $d$ hash tables are minimized when $\binom{m_1}{2}/n_1 = \ldots = \binom{m_d}{2}/n_d$. 
	$m_i$ and $n_i$ denote the elements inserted to table $i$ and the size of table $i$, respectively.  
\end{theorem}
\begin{proof}
	The amortized insertion complexity of a cuckoo hash is $O(1)$. Thus, like a balls and bins analysis, the expected number of conflicts occurring when inserting $m_i$ elements in table $i$ can be estimated as $\binom{m_i}{2}/n_i$. Minimizing the amortized conflicts among all hash tables can be modeled as the following optimization problem:
	\begin{equation}\label{eq:conflict-min}
	\begin{array}{ll@{}ll}
	\min_{m_1,\ldots,m_d \geq 0} & \sum_{i=1,\ldots,d} \binom{m_i}{2}/n_i \\
	\text{s.t.} & \sum_{i=1,\ldots,d} m_i = m
	\end{array}
	\end{equation}
	To solve the optimization problem, we establish an equivalent objective function:
	\begin{align*}
	\min \sum_{i=1,\ldots,d} \frac{\binom{m_i}{2}}{n_i} \Leftrightarrow \min \log(\frac{1}{d}\sum_{i=1,\ldots,d} \frac{\binom{m_i}{2}}{n_i})
	\end{align*}
	According to the Jensen's inequality, the following inequality holds:
	\begin{align*}
	\log(\frac{1}{d}\sum_{i=1,\ldots,d} \frac{\binom{m_i}{2}}{n_i}) \geq \frac{1}{d}\sum_{i=1,\ldots,d}\log(\frac{\binom{m_i}{2}}{n_i})
	\end{align*}
	where equality holds when $\binom{m_i}{2}/n_i = \binom{m_j}{2}/n_j$ $\forall i,j = 1,\ldots,d$ and we obtain the minimum.
\end{proof}

Based on our resizing strategy, one hash table can only be twice as large as the other tables. 
This implies that the filled factors of two tables are equal if they have the same size, i.e., $\theta_i = \theta_j$ if $n_i = n_j$, 
while $\theta_i \simeq \sqrt{2}\cdot \theta_j$ if $n_i = 2n_j$. 
Thus, larger tables should have a higher filled factor. 
Following Theorem~\ref{them:balance},
we employ a randomized approach: 
a KV pair $(k,v)$ is first assigned to table $i$ with a proportional probability to $n_i/\binom{m_i}{2}$ to ensure the distribution of KVs.

\subsection{Rehashing}\label{sec:dyn:rehash}
Whenever the filled factor falls out of the desired range, rehashing relocates KV pairs after one of the hash tables is resized. An efficient relocation process maximizes GPU device memory bandwidth and minimizes thread conflicts. 
We discuss two scenarios for rehashing: \emph{upsizing} and \emph{downsizing}, both of which are processed in a single kernel. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\textwidth]{fig/Upsize.pdf}
	\caption{Illustration for upsizing and downsizing.}
	\label{fig:upsize}
\end{figure}
\vspace{1mm}\noindent\textbf{Upsizing.} 
Here, we introduce a conflict-free rehashing strategy for the upsizing scenario. 
Figure~\ref{fig:upsize} illustrates the upsizing of a hash table $h^i$. 
As we always double the size for $h^i$, 
a KV pair that originally resides in bucket $loc$ could be rehashed to bucket $loc+|h^i|$ or stay in the original bucket. 
With this observation, we assign a warp for rehashing all KV pairs in the bucket to fully utilize the cache line size. 
Each thread in the warp takes a KV pair in the bucket and, if necessary, relocates that KV pair.
Moreover, rehashing does not trigger any conflicts as KV pairs from two distinct buckets before upsizing cannot be rehashed to the same bucket.  
Thus, locking of the bucket is not required, meaning we can make use of the device's full memory bandwidth for the upsizing process.  

After upsizing hash table $h^i$, its filled factor $\theta_i$ is cut in half, which could break the balancing condition emphasized in Theorem~\ref{them:balance}. Nevertheless, we use a sampling strategy for subsequent KV insertions, in which each insertion is allocated to table $i$ with a probability proportional to $n_i/\binom{m_i}{2}$, to recover the balancing condition. In particular, $m_i$ remains the same but $n_i$ doubles after upsizing, and the scenario leads to doubling the probability of inserting subsequent KV pairs to $h^i$. 


\vspace{1mm}\noindent\textbf{Downsizing.}
Downsizing $h^i$ is the reverse process of upsizing $h^i$. There is always room to relocate KV pairs in the same table for upsizing. 
However, downsizing may rehash some KV pairs to other hash tables, especially when $\theta_i > 50\%$.
Because the KV pairs located in $loc$ and $loc+|h^i|$ are hashed to $loc$ in the new table, there could be cases in which the KV pairs exceed the size of a single bucket. 
Hence, we first assign a warp to accommodate KV pairs that can fit the size of a single bucket. Like upsizing, downsizing does require locking as there will be no thread conflict on any bucket. 
For the remaining KV pairs that cannot fit in the downsized table, known as \emph{residuals}, we insert them into other subtables.
To ensure no conflict occurs between inserting residuals and processing the downsizing subtable, both of which are executed in a single kernel, we exclude the downsizing subtable when inserting the residuals. 
As an example when we have three subtables and one of them is being downsized, we only insert the residuals to the remaining two subtables. 


%To safeguard the balancing condition, we devise a different rehashing strategy for downsizing. 
%For all KV pairs in bucket $loc \geq |h^i|/2$ for table $i$ to be downsized, we assign a thread to rehash and reinsert a KV pair using Algorithm~\ref{algo:insert}. 
%In this way, we achieve the balancing condition at the expense of locking a bucket when reinserting a KV pair. 

\vspace{1mm}\noindent\textbf{Complexity Analysis.}
Given a total of $m$ elements in the hash tables, upsizing or downsizing rehashes at most $m/d$ KV pairs. 
To insert or delete these $m$ elements, the number of rehashes is bounded by $2m$.
Thus, the amortized complexity for inserting $m$ elements remains $O(1)$.
