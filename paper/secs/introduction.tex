\section{introduction}
The exceptional advances of General-Purpose Graphics Processing Units (GPGPUs) 
in recent years have completely revolutionized the computing paradigm across multiple fields, including cryptocurrency mining \cite{o2014bitcoin,taylor2013bitcoin}, machine learning \cite{coates2013deep,abadi2016tensorflow}, and database technologies \cite{bakkum2010accelerating,kaldewey2012gpu}.
GPUs bring astonishing computational power that is only available from supercomputers in the past. 
The state-of-the-art commercial GPU equipment (GV100) is capable to operate at the speed of 14.8 TFLOPS for single precision arithmetic and 870 GB/s peak memory bandwidth \footnote{Quadro GV100 launched by NVIDIA in March 2018}. 
Meanwhile, as the computing resources of GPUs continue to explode, it makes rooms for more applications to run on a single GPU equipment concurrently. For example, GV100 is built with 5120 CUDA cores and 32GB device memory. Nevertheless, most GPU programs try to occupy as much resources as possible to maximize their performance individually. 
The egoism renders inefficiency when the GPUs run multiple programs simultaneously. Imaging a number of concurrent programs requesting a total memory size larger than the device memory. Interleaving the program executions leads to redundant data transfers between CPUs and GPUs through PCIe, which is expensive in nature. 

In this paper, we investigate a fundamental data structure, i.e., \emph{hash table}, which has been implemented on GPUs to accelerate numerous applications, ranging from relational hash join \cite{he2008relational,he2009relational,heimel2013hardware}, data mining \cite{pan2011fast,zhou2010parallel,zhong2014medusa},  key value store \cite{zhang2015mega,hetherington2015memcachedgpu,breslow2016horton} and many others \cite{bowers2010parallel,pan2010efficient,garcia2011coherent,niessner2013real,wu2015gpu}. Existing works \cite{alcantara2009real,zhang2015mega,hong2010mapcg,hetherington2015memcachedgpu,breslow2016horton} focus on the static scenario: they know the data size in advance and allocate a sufficiently large hash table to insert all data entries efficiently. In many cases, the data size varies and static allocation leads to poor device memory utilization. To fill this gap, we design a dynamic GPU hash table that adaptively adjusts to the size of active entries in the table. The hash table supports efficient memory management by sustaining a guaranteed \emph{filled factor} of the table when the data size changes. There are two major challenges for maintaining a high filled factor for hash tables on GPUs:

\begin{itemize}
	\item It leads to a smaller hash table size with less distinct keys, hence triggers additional conflicts as multiple threads trying to insert/delete the data, which is particularly expensive under the GPU architecture;
	\item It also means that the table needs to be frequently adjusted to the active data size and we incur costly rehashing to move the entries to a new table when the old table cannot accommodate the adjusted data. 
\end{itemize}

To overcome the aforementioned challenges, we propose a dynamic cuckoo hash table on GPUs. Cuckoo hashing \cite{pagh2004cuckoo} uses a number of hash functions to provide each key with multiple locations instead of one. When a location is occupied, existing key is relocated to make room for the new one. Existing works \cite{alcantara2009real,alcantara2011building,zhang2015mega,breslow2016horton} have demonstrated great success in speeding up their respective applications by paralleling cuckoo hash on GPUs. 
However, most of these works require the size of a Key-Value (KV) pair to fit a single atomic transaction on GPUs (64 bits wide) to handle conflicts when multiple threads trying to update keys hashed to the same value.
%They also build on inefficient locking mechanisms to handle conflicts, which severely downgrade the performance when high contention occurs. 
In addition, a complete relocation of the entire hash table is required when the data cannot be completely inserted. 
It thus calls for a general hash table design to support larger KV size as well as efficient relocation strategy against dynamic updates.
In this work, we offer two novel designs for implementing dynamic cuckoo hash tables on GPUs. 

First, we propose a voter-based coordination scheme among massive GPU threads to support efficient locking without assuming the size of the key-value pairs.
For each hash value, we allocate a bucket of $b$ locations to store key-value pairs. 
Each thread is assigned to an insertion operation on one key. Instead of immediately acquiring a lock on the corresponding bucket to be updated, a thread will first propose a vote among its warp group and all threads in that same warp collaborate to join the winner thread for its update task. There are three distinguishing advantages for the voter-based coordination: (a) once a conflict is detected on one bucket, instead of spinning, the warp instantly revotes and switches to another bucket; (b) a near-optimal load balancing is achieved as a thread will assist other warp-aligned threads, even when the thread finishes its assigned tasks; (c) locking the bucket exclusively allows to update KV pairs without assuming their length below 64 bits.

Second, we employ the cuckoo hashing scheme with $d$ subtables specified by $d$ hash functions, and introduce a resizing policy to maintain filled factor in a bounded range, while minimizing entries in all subtables relocated at the same time. Insertions and deletions would trigger the hash tables to grow and shrink, if the filled factor falls out of the specified range. 
Our proposed policy only lock one subtable for resizing and it always ensures no subtable can be more than twice as large as any other for handling subsequent resizing efficiently. Meanwhile, the entries in the hash table are distributed so that each subtable has near-equivalent filled factor.
In this way, we drastically reduce the cost of resizing the hash tables and provide better system availability compared with existing works that needs to relocate all data for resizing.
Our theoretical analysis demonstrates the optimality of the scheduling policy in terms of processing updates. 
Empirically, the proposed hash table design is capable to operate efficiently at filled factors exceeding 90\%.

Hereby, we summarize our contributions as follows:
\begin{itemize}
	\item We propose a general dynamic hash table without assuming the size of the key-value pair and devise a novel voter-based coordination scheme to support the locking mechanism under massive GPU threads. 
	\item We introduce an efficient policy to resize the hash tables and our theoretical analysis has demonstrated the near-optimality of the resizing policy.
	\item We conduct extensive experiments on both synthetic and real datasets to showcase the superiority of the proposed approach over several state-of-the-art baselines on GPU hashing. 
	For guaranteed filled factor, our hash table design achieves up to 5x speedups over the baselines against dynamic workloads. The profiling results have also revealed that our approach efficiently leverages GPU resources and demonstrates near-optimal load balancing. 
\end{itemize}

The remaining part of this paper is organized as follows. Section~\ref{sec:pre} introduces the preliminaries and the background on GPUs, followed by the related work in Section~\ref{sec:rel}. Section~\ref{sec:vot} presents our hash table designs as well as the voter-based coordination scheme.  Section~\ref{sec:dyn} introduces our resizing policy against dynamic updates on hash tables. The experimental results are reported in Section~\ref{sec:exp}. Finally, we conclude the paper in Section~\ref{sec:con}. 